{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe904c30",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b101d40a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T02:43:46.736191Z",
     "start_time": "2022-04-02T02:43:46.727704Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, time\n",
    "import random\n",
    "from IPython.display import clear_output as clear\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import gzip\n",
    "\n",
    "import csv\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1772c51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T02:21:40.261962Z",
     "start_time": "2022-04-02T02:21:25.942491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9595\n"
     ]
    }
   ],
   "source": [
    "pce = []\n",
    "SMILES = []\n",
    "for data_split in os.listdir('./data'):\n",
    "    data_split = './data/'+data_split\n",
    "    SMILES += pd.read_csv(data_split)['SMILES_str'].tolist()\n",
    "    pce += pd.read_csv(data_split)['pce'].tolist()    \n",
    "SMILES = np.array(SMILES)\n",
    "\n",
    "rdkit_ok_index = pd.read_csv(\"./index_rdkit_ok.csv\").iloc[:,0].tolist()\n",
    "random.seed(a=1)\n",
    "indexs = random.sample(rdkit_ok_index, 10000)\n",
    "SMILES_sample = [SMILES[index] for index in indexs]\n",
    "pce_sample = [pce[index] for index in indexs]\n",
    "index_0 = []\n",
    "for i in indexs:\n",
    "    if pce[i]==0:\n",
    "        index_0.append(i)\n",
    "        \n",
    "index_0_91 = random.sample(index_0, 91)\n",
    "\n",
    "index_ban = []\n",
    "for i in indexs:\n",
    "    if i in index_0:\n",
    "        if i in index_0_91:\n",
    "            index_ban.append(i)\n",
    "    else:\n",
    "        index_ban.append(i)\n",
    "print(len(index_ban))\n",
    "\n",
    "SMILES_sample_ban = [SMILES[index] for index in index_ban]\n",
    "pce_sample_ban = [pce[index] for index in index_ban]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b2edf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T02:13:57.958877Z",
     "start_time": "2022-04-02T02:13:57.922945Z"
    }
   },
   "outputs": [],
   "source": [
    "# RNN k_flod \n",
    "def data_train_kf(data, hidden_size=200, n_layers=2, n_epoch=100, batch_size=16, USE_GPU = False, set_cv=5):\n",
    "    def createTensor(tensor):\n",
    "        if USE_GPU:\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            tensor = tensor.to(device)\n",
    "        return tensor\n",
    "\n",
    "    class RNNClassifier(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size, num_layers = 1, bidirectional = True):\n",
    "            super(RNNClassifier, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "            self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, bidirectional = bidirectional)\n",
    "\n",
    "            self.fc = torch.nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "        def initHidden(self, batch_size):\n",
    "            hidden = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "            return createTensor(hidden)\n",
    "\n",
    "        def forward(self, input, seq_lengths):\n",
    "            input = input.t()\n",
    "            batch_size = input.size(1)\n",
    "            hidden = self.initHidden(batch_size)\n",
    "\n",
    "            embedding = self.embedding(input)\n",
    "\n",
    "            gru_input = pack_padded_sequence(embedding, seq_lengths)\n",
    "            output, hidden = self.gru(gru_input, hidden)\n",
    "            if self.num_directions == 2:\n",
    "                hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim = 1)\n",
    "            else:\n",
    "                hidden_cat = hidden[-1]\n",
    "\n",
    "            fc_output = self.fc(hidden_cat)\n",
    "            return fc_output\n",
    "\n",
    "    def smiles_to_ASCIIlist(smiles_list):\n",
    "        ASCIIlist = [ord(smiles) for smiles in smiles_list]\n",
    "        return ASCIIlist\n",
    "\n",
    "    def makeTensors(smiles_list, tox_list):\n",
    "        smiles_sequences = [smiles_to_ASCIIlist(smiles) for smiles in smiles_list]\n",
    "        smiles_seq_lens = torch.LongTensor([len(name_ASCII) for name_ASCII in smiles_sequences])\n",
    "\n",
    "        smiles_tensor = torch.zeros(len(smiles_sequences), smiles_seq_lens.max()).long()\n",
    "        for index, (smiles_sequence, smiles_seq_len) in enumerate(zip(smiles_sequences, smiles_seq_lens), 0):\n",
    "            smiles_tensor[index, 0:smiles_seq_len] = torch.LongTensor(smiles_sequence)\n",
    "\n",
    "        ordered_smiles_seq_lens, len_indexes = smiles_seq_lens.sort(dim = 0, descending = True)\n",
    "        ordered_smiles_tensor = smiles_tensor[len_indexes]\n",
    "        ordered_tox_list = tox_list[len_indexes]\n",
    "\n",
    "        return createTensor(ordered_smiles_tensor), createTensor(ordered_smiles_seq_lens), createTensor(ordered_tox_list) \n",
    "\n",
    "    def train():\n",
    "        loss = 0.0\n",
    "        for batch_index, (smiles, tox) in enumerate(train_loader):\n",
    "\n",
    "            inputs, seq_lens, targets = makeTensors(smiles, tox)\n",
    "            if USE_GPU:\n",
    "                outputs = classifier_model(inputs, seq_lens.cpu())\n",
    "            else:\n",
    "                outputs = classifier_model(inputs, seq_lens)\n",
    "            targets = targets.to(torch.float32).reshape(-1,1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += loss.item()\n",
    "\n",
    "            if batch_index % 50 == 49:\n",
    "                print('time_elapsed: {:.1f}m {:.1f}s, Epoch {}, '.format(timePassed(start_time)[0], timePassed(start_time)[1], epoch), end = '')\n",
    "                print(f'[{(batch_index+1) * len(inputs)} / {len(training_set)}] ', end = '')\n",
    "                print(f'loss = {loss / ((batch_index+1) * len(inputs))}')\n",
    "\n",
    "        return loss/len(train_loader)\n",
    "\n",
    "    def test():\n",
    "        correct = 0\n",
    "        total_samples = len(test_set)\n",
    "        with torch.no_grad():\n",
    "            pred = []\n",
    "            obs = []\n",
    "            for i, (smiles, tox) in enumerate(test_loader):\n",
    "                inputs, seq_lens, targets = makeTensors(smiles, tox)\n",
    "                if USE_GPU:\n",
    "                    outputs = classifier_model(inputs, seq_lens.cpu())\n",
    "                    pred.append(outputs.data.cpu())\n",
    "                    obs.append(targets.data.reshape(-1,1).cpu())\n",
    "                else:\n",
    "                    outputs = classifier_model(inputs, seq_lens)\n",
    "                    pred.append(outputs.data)\n",
    "                    obs.append(targets.data.reshape(-1,1))\n",
    "            pred = np.vstack(pred).ravel()\n",
    "            obs = np.vstack(obs).ravel()\n",
    "            r2 = r2_score(obs, pred)\n",
    "            Mse = mse(obs, pred)\n",
    "            print('r2 on test: %.3f %%' % (100 * r2), 'mse on test: %.5f \\n' % (Mse))\n",
    "        return r2, Mse, pred, obs\n",
    "\n",
    "    def timePassed(start_time):\n",
    "        time_passed = time.time() - start_time\n",
    "        minute = math.floor(time_passed / 60)\n",
    "        second = time_passed - minute * 60\n",
    "        return [minute, second]\n",
    "\n",
    "    os.makedirs(\"../data_vf_tp/{}/results/DL/1/RNN/test/\".format(data), exist_ok=True)\n",
    "    os.makedirs(\"../data_vf_tp/{}/results/DL/1/RNN/train/\".format(data), exist_ok=True)\n",
    "    train_smiles = SMILES_sample[:8000]\n",
    "    train_tox = [float(i) for i in pce_sample[:8000]]\n",
    "    training_set = [(train_smiles[i], train_tox[i]) for i in range(len(train_smiles))]\n",
    "\n",
    "    test_smiles = SMILES_sample[8000:10000]\n",
    "    test_tox = [float(i) for i in pce_sample[8000:10000]]\n",
    "    test_set = [(test_smiles[i], test_tox[i]) for i in range(len(test_smiles))]\n",
    "\n",
    "    seed = 1\n",
    "    base_indices = np.arange(0,len(train_tox))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(base_indices)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(base_indices)\n",
    "    \n",
    "    step = int(len(train_tox)/set_cv)\n",
    "    pred_kflod = pd.DataFrame(index=range(len(train_tox)), columns=[\"cv1~{}\".format(set_cv), \"True\"])\n",
    "\n",
    "    for cv in range(set_cv):\n",
    "        print(\"*\"*20, \"Kflod\", cv ,\"*\"*20)\n",
    "        index = base_indices\n",
    "        if cv < set_cv-1:\n",
    "            index_train = np.concatenate([index[:cv*step],index[(cv+1)*step:]], axis=0)\n",
    "            index_val = index[cv*step:(cv+1)*step]\n",
    "        else: \n",
    "            index_train = index[0:cv*step]\n",
    "            index_val = index[cv*step:]\n",
    "\n",
    "        # parameters\n",
    "        n_chars = 0\n",
    "        for smiles in train_smiles:\n",
    "            for char in smiles:\n",
    "                if n_chars < ord(char):\n",
    "                    n_chars = ord(char)\n",
    "        for smiles in test_smiles:\n",
    "            for char in smiles:\n",
    "                if n_chars < ord(char):\n",
    "                    n_chars = ord(char)\n",
    "        n_chars = n_chars+1\n",
    "        output_size = 1\n",
    "        train_loader = DataLoader(dataset = [training_set[index] for index in index_train], batch_size = batch_size, shuffle = True)\n",
    "        test_loader = DataLoader(dataset = [training_set[index] for index in index_val], batch_size = batch_size, shuffle = False)\n",
    "\n",
    "        classifier_model = RNNClassifier(n_chars, hidden_size, output_size, n_layers) # input_size, hidden_size, output_size, num_layers\n",
    "        if USE_GPU:\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            classifier_model.to(device)\n",
    "\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(classifier_model.parameters(), lr = 0.001, weight_decay=10 ** (-5.0))\n",
    "\n",
    "        # training and test\n",
    "        start_time = time.time()\n",
    "        print(\"The num of total training epochs is %d. \" % n_epoch)\n",
    "        r2_list = []\n",
    "        Mse_list = []\n",
    "        best_r2 = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            train()\n",
    "            r2, Mse, pred, obs = test()\n",
    "            r2_list.append(r2)\n",
    "            Mse_list.append(Mse)\n",
    "            if best_r2 < r2:\n",
    "                best_r2 = r2\n",
    "                pred_kflod.iloc[index_val,0] = pred.reshape(1,-1)\n",
    "                pred_kflod.iloc[index_val,1] = obs.reshape(1,-1)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(1, n_epoch+1), r2_list)\n",
    "        plt.plot(np.arange(1, n_epoch+1), Mse_list)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"r2 & MSE\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    display(pred_kflod)\n",
    "    pred_kflod.to_csv(\"../data_vf_tp/{}/results/DL/1/RNN/train/kf_pred_all.csv\".format(data), index=0)\n",
    "    print(r2_score(pred_kflod.iloc[:,1], pred_kflod.iloc[:,0]))\n",
    "\n",
    "data_train_kf(\"or_pho_pce\", hidden_size=200, n_layers=2, n_epoch=300, batch_size=32, USE_GPU = False, set_cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ee4b65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T08:09:05.210751Z",
     "start_time": "2022-04-02T08:09:05.180064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\kekule\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# RNN test\n",
    "def data_train_test(data, hidden_size=200, n_layers=2, n_epoch=100, batch_size=16, USE_GPU = False):\n",
    "    \n",
    "    def createTensor(tensor):\n",
    "        if USE_GPU:\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            tensor = tensor.to(device)\n",
    "        return tensor\n",
    "\n",
    "    class RNNClassifier(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size, num_layers = 1, bidirectional = True):\n",
    "            super(RNNClassifier, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.num_directions = 2 if bidirectional else 1\n",
    "            self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "            self.gru = torch.nn.GRU(hidden_size, hidden_size, num_layers, bidirectional = bidirectional)\n",
    "\n",
    "            self.fc = torch.nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "        def initHidden(self, batch_size):\n",
    "            hidden = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size)\n",
    "            return createTensor(hidden)\n",
    "\n",
    "        def forward(self, input, seq_lengths):\n",
    "            input = input.t()\n",
    "            batch_size = input.size(1)\n",
    "            hidden = self.initHidden(batch_size)\n",
    "\n",
    "            # embedding layer\n",
    "            embedding = self.embedding(input)\n",
    "            # GRU\n",
    "            gru_input = pack_padded_sequence(embedding, seq_lengths)\n",
    "            output, hidden = self.gru(gru_input, hidden)\n",
    "            if self.num_directions == 2:\n",
    "                hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim = 1)\n",
    "            else:\n",
    "                hidden_cat = hidden[-1] \n",
    "\n",
    "            fc_output = self.fc(hidden_cat)\n",
    "            return fc_output\n",
    "\n",
    "    def smiles_to_ASCIIlist(smiles_list):\n",
    "        ASCIIlist = [ord(smiles) for smiles in smiles_list]\n",
    "        return ASCIIlist\n",
    "\n",
    "    def makeTensors(smiles_list, tox_list):\n",
    "        smiles_sequences = [smiles_to_ASCIIlist(smiles) for smiles in smiles_list]\n",
    "        smiles_seq_lens = torch.LongTensor([len(name_ASCII) for name_ASCII in smiles_sequences])\n",
    "\n",
    "        smiles_tensor = torch.zeros(len(smiles_sequences), smiles_seq_lens.max()).long()\n",
    "        for index, (smiles_sequence, smiles_seq_len) in enumerate(zip(smiles_sequences, smiles_seq_lens), 0):\n",
    "            smiles_tensor[index, 0:smiles_seq_len] = torch.LongTensor(smiles_sequence)\n",
    "\n",
    "        ordered_smiles_seq_lens, len_indexes = smiles_seq_lens.sort(dim = 0, descending = True)\n",
    "        ordered_smiles_tensor = smiles_tensor[len_indexes]\n",
    "        ordered_tox_list = tox_list[len_indexes]\n",
    "\n",
    "        return createTensor(ordered_smiles_tensor), createTensor(ordered_smiles_seq_lens), createTensor(ordered_tox_list) \n",
    "\n",
    "    def train():\n",
    "        loss = 0.0\n",
    "        for batch_index, (smiles, tox) in enumerate(train_loader): \n",
    "\n",
    "            inputs, seq_lens, targets = makeTensors(smiles, tox) \n",
    "            if USE_GPU:\n",
    "                outputs = classifier_model(inputs, seq_lens.cpu())\n",
    "            else:\n",
    "                outputs = classifier_model(inputs, seq_lens)\n",
    "            targets = targets.to(torch.float32).reshape(-1,1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += loss.item()\n",
    "\n",
    "        return loss/len(train_loader)\n",
    "\n",
    "    def test():\n",
    "        correct = 0\n",
    "        total_samples = len(test_set)\n",
    "        with torch.no_grad():\n",
    "            pred = []\n",
    "            obs = []\n",
    "            for i, (smiles, tox) in enumerate(test_loader):\n",
    "                inputs, seq_lens, targets = makeTensors(smiles, tox)\n",
    "                if USE_GPU:\n",
    "                    outputs = classifier_model(inputs, seq_lens.cpu())\n",
    "                    pred.append(outputs.data.cpu())\n",
    "                    obs.append(targets.data.reshape(-1,1).cpu())\n",
    "                else:\n",
    "                    outputs = classifier_model(inputs, seq_lens)\n",
    "                    pred.append(outputs.data)\n",
    "                    obs.append(targets.data.reshape(-1,1))\n",
    "            pred = np.vstack(pred).ravel()\n",
    "            obs = np.vstack(obs).ravel()\n",
    "            r2 = r2_score(obs, pred)\n",
    "            Mse = mse(obs, pred)\n",
    "        return r2, Mse, pred, obs\n",
    "\n",
    "    def timePassed(start_time):\n",
    "        time_passed = time.time() - start_time\n",
    "        minute = math.floor(time_passed / 60)\n",
    "        second = time_passed - minute * 60\n",
    "        return [minute, second]\n",
    "\n",
    "    os.makedirs(\"./results/DL/RNN/test/\".format(data), exist_ok=True)\n",
    "    os.makedirs(\"./results/DL/RNN/train/\".format(data), exist_ok=True)\n",
    "\n",
    "    train_smiles = SMILES_sample[:8000]\n",
    "    train_tox = [float(i) for i in pce_sample[:8000]]\n",
    "    training_set = [(train_smiles[i], train_tox[i]) for i in range(len(train_smiles))]\n",
    "\n",
    "    test_smiles = SMILES_sample[8000:10000]\n",
    "    test_tox = [float(i) for i in pce_sample[8000:10000]]\n",
    "    test_set = [(test_smiles[i], test_tox[i]) for i in range(len(test_smiles))]\n",
    "\n",
    "    n_chars = 0\n",
    "    for smiles in train_smiles:\n",
    "        for char in smiles:\n",
    "            if n_chars < ord(char):\n",
    "                n_chars = ord(char)\n",
    "    for smiles in test_smiles:\n",
    "        for char in smiles:\n",
    "            if n_chars < ord(char):\n",
    "                n_chars = ord(char)\n",
    "    n_chars = n_chars+1\n",
    "    output_size = 1\n",
    "    \n",
    "    train_loader = DataLoader(dataset = training_set, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "    classifier_model = RNNClassifier(n_chars, hidden_size, output_size, n_layers) # input_size, hidden_size, output_size, num_layers\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier_model.to(device)\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr = 0.001, weight_decay=10 ** (-5.0))\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"The num of total training epochs is %d. \" % n_epoch)\n",
    "    r2_list = []\n",
    "    Mse_list = []\n",
    "    best_r2 = 0\n",
    "    train_loss = []\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss.append(train())\n",
    "        r2, Mse, pred, obs = test()\n",
    "        r2_list.append(r2)\n",
    "        Mse_list.append(Mse)\n",
    "        if best_r2 < r2:\n",
    "            best_r2 = r2\n",
    "            pd.DataFrame([pred, obs], index=[\"pred\", \"true\"]).T.to_csv('./results/DL/RNN/test/test_pred.csv'.format(data), index=0)\n",
    "            \n",
    "        # display\n",
    "        clear()\n",
    "        plt.figure(figsize=[16,9])\n",
    "        plt.plot(np.arange(1, epoch+2), r2_list, label='test_r2')\n",
    "        plt.plot(np.arange(1, epoch+2), Mse_list, label='test_mse')\n",
    "        plt.plot(np.arange(1, epoch+2), train_loss, label='train_mse')\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"r2 & MSE\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        for i in range(epoch+1):\n",
    "            print('r2 on test: %.3f %%' % (100 * r2_list[i]), 'mse on test: %.5f \\n' % (Mse_list[i]), 'mse on train: %.5f \\n' % (train_loss[i]))\n",
    "    \n",
    "    pd.DataFrame(r2_list).to_csv(\"./results/DL/RNN/test/scores_r2.csv\".format(data), index=0)\n",
    "    pd.DataFrame(Mse_list).to_csv(\"./results/DL/RNN/test/scores_mse.csv\".format(data), index=0)\n",
    "    pd.DataFrame(train_loss).to_csv(\"./results/DL/RNN/test/train_loss.csv\".format(data), index=0)\n",
    "    \n",
    "    print(r2_list.index(max(r2_list)))\n",
    "    print(max(r2_list))\n",
    "    \n",
    "data_train_test(\"or_pho_pce\", hidden_size=200, n_layers=2, n_epoch=300, batch_size=32, USE_GPU = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e7e27",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed135a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T02:29:04.669131Z",
     "start_time": "2022-04-02T02:29:03.824329Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit import RDPaths\n",
    "from rdkit.Chem import rdmolops, rdmolfiles\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from dgl import model_zoo\n",
    "\n",
    "from dgl.data.chem.utils import mol_to_complete_graph, mol_to_bigraph\n",
    "# from dgllife.utils import mol_to_bigraph\n",
    "\n",
    "from dgl.data.chem.utils import atom_type_one_hot\n",
    "from dgl.data.chem.utils import atom_degree_one_hot\n",
    "from dgl.data.chem.utils import atom_formal_charge\n",
    "from dgl.data.chem.utils import atom_num_radical_electrons\n",
    "from dgl.data.chem.utils import atom_hybridization_one_hot\n",
    "from dgl.data.chem.utils import atom_total_num_H_one_hot\n",
    "from dgl.data.chem.utils import one_hot_encoding\n",
    "from dgl.data.chem import CanonicalAtomFeaturizer\n",
    "from dgl.data.chem import CanonicalBondFeaturizer\n",
    "from dgl.data.chem import ConcatFeaturizer\n",
    "from dgl.data.chem import BaseAtomFeaturizer\n",
    "from dgl.data.chem import BaseBondFeaturizer\n",
    "\n",
    "from dgl.data.chem import one_hot_encoding\n",
    "from dgl.data.utils import split_dataset\n",
    "\n",
    "\n",
    "# from dgllife.utils import one_hot_encoding, atom_type_one_hot, atom_formal_charge, atom_num_radical_electrons \n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a30995f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T08:09:17.450828Z",
     "start_time": "2022-04-02T08:09:17.433873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\kekule\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# GNN k_fold\n",
    "set_cv = 5\n",
    "n_epochs = 200\n",
    "\n",
    "def chirality(atom):\n",
    "    try:\n",
    "        return one_hot_encoding(atom.GetProp('_CIPCode'), ['R', 'S']) + \\\n",
    "               [atom.HasProp('_ChiralityPossible')]\n",
    "    except:\n",
    "        return [False, False] + [atom.HasProp('_ChiralityPossible')]\n",
    "\n",
    "\n",
    "def collate_molgraphs(data):\n",
    "    assert len(data[0]) in [3, 4], \\\n",
    "        'Expect the tuple to be of length 3 or 4, got {:d}'.format(len(data[0]))\n",
    "    if len(data[0]) == 3:\n",
    "        smiles, graphs, labels = map(list, zip(*data))\n",
    "        masks = None\n",
    "    else:\n",
    "        smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "    bg = dgl.batch(graphs)\n",
    "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    if masks is None:\n",
    "        masks = torch.ones(labels.shape)\n",
    "    else:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    return smiles, bg, labels, masks\n",
    "\n",
    "\n",
    "def run_a_train_epoch(n_epochs, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    pred = []\n",
    "    best_r2 = 0\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        batch_data\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        bg.to(torch.device('cpu'))\n",
    "        labels = labels.to('cpu')\n",
    "        masks = masks.to('cpu')\n",
    "\n",
    "        prediction = model(bg, bg.ndata['hv'], bg.edata['he'])\n",
    "        pred.append(prediction.detach().numpy())\n",
    "        loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.data.item())\n",
    "\n",
    "    model.eval()\n",
    "    all_pred = []\n",
    "    all_y = []\n",
    "    for test_data in test_loader:\n",
    "        smi_lst, bg, labels, masks = test_data\n",
    "        bg.to(torch.device('cpu'))\n",
    "        labels = labels.to('cpu')\n",
    "        masks = masks.to('cpu')\n",
    "        pred = model(bg, bg.ndata['hv'], bg.edata['he'])\n",
    "        all_pred.append(pred.data.cpu().numpy())\n",
    "        all_y.append(labels)\n",
    "\n",
    "    res = np.vstack(all_pred)\n",
    "    r2 = r2_score(np.vstack(all_y), np.vstack(res))\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        pred_kflod.iloc[index_val, 0] = np.ravel(res)\n",
    "        pred_kflod.iloc[index_val, 1] = np.ravel(np.vstack(all_y))\n",
    "\n",
    "    Mse = mse(np.vstack(all_y), np.vstack(res))\n",
    "    r2_all.append(r2)\n",
    "    mse_all.append(Mse)\n",
    "    total_score = np.mean(losses)\n",
    "    print('epoch {:d}/{:d}, cv: {} train mse: {:.4f} test r_2: {:.4f} mse: {:.4f}'.format(epoch + 1, n_epochs, cv, total_score, r2, Mse), end=\"\")\n",
    "    return total_score\n",
    "\n",
    "atom_featurizer = BaseAtomFeaturizer(\n",
    "                 {'hv': ConcatFeaturizer([\n",
    "                  partial(atom_type_one_hot, allowable_set=[\n",
    "                          'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At'],\n",
    "                    encode_unknown=True),\n",
    "                  partial(atom_degree_one_hot, allowable_set=list(range(6))),\n",
    "                  atom_formal_charge, atom_num_radical_electrons,\n",
    "                  partial(atom_hybridization_one_hot, encode_unknown=True),\n",
    "                  lambda atom: [0], # A placeholder for aromatic information,\n",
    "                    atom_total_num_H_one_hot, chirality\n",
    "                 ],\n",
    "                )})\n",
    "bond_featurizer = BaseBondFeaturizer({\n",
    "                                     'he': lambda bond: [0 for _ in range(10)]\n",
    "    })\n",
    "\n",
    "os.makedirs(\"./results/DL/GNN/test/\", exist_ok=True)\n",
    "os.makedirs(\"./results/DL/GNN/train/\", exist_ok=True)\n",
    "\n",
    "train_smi = SMILES_sample[:8000]\n",
    "train_mols = [Chem.MolFromSmiles(smiles) for smiles in train_smi]\n",
    "train_sol = torch.tensor(pce_sample[:8000]).reshape(-1, 1)\n",
    "\n",
    "test_smi = SMILES_sample[8000:10000]\n",
    "test_mols = [Chem.MolFromSmiles(smiles) for smiles in test_smi]\n",
    "test_sol = torch.tensor(pce_sample[8000:10000]).reshape(-1, 1)\n",
    "\n",
    "train_graph = [mol_to_bigraph(mol,\n",
    "                              node_featurizer=atom_featurizer,\n",
    "                              edge_featurizer=bond_featurizer) for mol in train_mols]\n",
    "\n",
    "test_graph = [mol_to_bigraph(mol,\n",
    "                             node_featurizer=atom_featurizer,\n",
    "                             edge_featurizer=bond_featurizer) for mol in test_mols]\n",
    "\n",
    "model = model_zoo.chem.AttentiveFP(node_feat_size=39,\n",
    "                                   edge_feat_size=10,\n",
    "                                   num_layers=2,\n",
    "                                   num_timesteps=2,\n",
    "                                   graph_feat_size=200,\n",
    "                                   output_size=1,\n",
    "                                   dropout=0.2)\n",
    "model = model.to('cpu')\n",
    "\n",
    "seed = 1\n",
    "base_indices = np.arange(0,len(train_sol))\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(base_indices)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(base_indices)\n",
    "step = int(len(train_sol)/set_cv)\n",
    "pred_kflod = pd.DataFrame(index=range(len(train_sol)), columns=[\"cv1~5\", \"True\"])\n",
    "\n",
    "clear_output()\n",
    "\n",
    "for cv in range(set_cv):\n",
    "    print(\"*\"*20, \"Kflod\", cv ,\"*\"*20)\n",
    "    index = base_indices\n",
    "    if cv < set_cv-1:\n",
    "        index_train = np.concatenate([index[:cv*step],index[(cv+1)*step:]], axis=0)\n",
    "        index_val = index[cv*step:(cv+1)*step]\n",
    "    else: \n",
    "        index_train = index[0:cv*step]\n",
    "        index_val = index[cv*step:]\n",
    "\n",
    "    train_loader = DataLoader(dataset=list(zip(np.array(train_smi)[index_train], np.array(train_graph)[index_train], train_sol[index_train])), batch_size=122,\n",
    "                              collate_fn=collate_molgraphs)\n",
    "    test_loader = DataLoader(dataset=list(zip(np.array(train_smi)[index_val], np.array(train_graph)[index_val], train_sol[index_val])), batch_size=122,\n",
    "                             collate_fn=collate_molgraphs)\n",
    "\n",
    "    model = model_zoo.chem.AttentiveFP(node_feat_size=39,\n",
    "                                   edge_feat_size=10,\n",
    "                                   num_layers=2,\n",
    "                                   num_timesteps=2,\n",
    "                                   graph_feat_size=200,\n",
    "                                   output_size=1,\n",
    "                                   dropout=0.2)\n",
    "    model = model.to('cpu')\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=10 ** (-2.5), weight_decay=10 ** (-5.0),)\n",
    "\n",
    "    epochs = []\n",
    "    scores = []\n",
    "    r2_all = []\n",
    "    mse_all = []\n",
    "    for e in range(n_epochs):\n",
    "        score = run_a_train_epoch(n_epochs, e, model, train_loader, loss_fn, optimizer)\n",
    "        epochs.append(e)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.plot(epochs, scores)\n",
    "    plt.savefig(f'./results/{cv}cv_losses.png')\n",
    "    pd.DataFrame(scores).to_csv(f'./results/{cv}cv_losses.csv')\n",
    "\n",
    "print(r2_score(pred_kflod.iloc[:,0], pred_kflod.iloc[:,1]))\n",
    "pred_kflod.to_csv(\"./results/DL/GNN/train/kf_pred_all.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84f46f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T08:08:54.360136Z",
     "start_time": "2022-04-02T08:08:51.825Z"
    }
   },
   "outputs": [],
   "source": [
    "# GNN test\n",
    "def data_train_test(data, n_epochs = 100):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    def chirality(atom):\n",
    "        try:\n",
    "            return one_hot_encoding(atom.GetProp('_CIPCode'), ['R', 'S']) + \\\n",
    "                   [atom.HasProp('_ChiralityPossible')]\n",
    "        except:\n",
    "            return [False, False] + [atom.HasProp('_ChiralityPossible')]\n",
    "\n",
    "\n",
    "    def collate_molgraphs(data):\n",
    "        assert len(data[0]) in [3, 4], \\\n",
    "            'Expect the tuple to be of length 3 or 4, got {:d}'.format(len(data[0]))\n",
    "        if len(data[0]) == 3:\n",
    "            smiles, graphs, labels = map(list, zip(*data))\n",
    "            masks = None\n",
    "        else:\n",
    "            smiles, graphs, labels, masks = map(list, zip(*data))\n",
    "\n",
    "        bg = dgl.batch(graphs)\n",
    "        bg.set_n_initializer(dgl.init.zero_initializer)\n",
    "        bg.set_e_initializer(dgl.init.zero_initializer)\n",
    "        labels = torch.stack(labels, dim=0)\n",
    "\n",
    "        if masks is None:\n",
    "            masks = torch.ones(labels.shape)\n",
    "        else:\n",
    "            masks = torch.stack(masks, dim=0)\n",
    "        return smiles, bg, labels, masks\n",
    "\n",
    "\n",
    "    def run_a_train_epoch(n_epochs, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        losses = []\n",
    "        pred = []\n",
    "        best_r2 = 0\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            batch_data\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            bg.to(torch.device('cpu'))\n",
    "            labels = labels.to('cpu')\n",
    "            masks = masks.to('cpu')\n",
    "\n",
    "            prediction = model(bg, bg.ndata['hv'], bg.edata['he'])\n",
    "            pred.append(prediction.detach().numpy())\n",
    "            loss = (loss_criterion(prediction, labels) * (masks != 0).float()).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "        model.eval()\n",
    "        all_pred = []\n",
    "        for test_data in test_loader:\n",
    "            smi_lst, bg, labels, masks = test_data\n",
    "            bg.to(torch.device('cpu'))\n",
    "            labels = labels.to('cpu')\n",
    "            masks = masks.to('cpu')\n",
    "            pred = model(bg, bg.ndata['hv'], bg.edata['he'])\n",
    "            all_pred.append(pred.data.cpu().numpy())\n",
    "\n",
    "        res = np.vstack(all_pred)\n",
    "        r2 = r2_score(test_sol, np.vstack(res))\n",
    "        Mse = MSE(test_sol, np.vstack(res))\n",
    "        Mae = MAE(test_sol, np.vstack(res))\n",
    "        r2_all.append(r2)\n",
    "        mse_all.append(Mse)\n",
    "        mae_all.append(Mae)\n",
    "        if best_r2 < r2:\n",
    "            best_r2 = r2\n",
    "            print()\n",
    "#             print(res)\n",
    "            pd.DataFrame([res.ravel(), np.array(test_sol).ravel()]).T.to_csv(\"./results/DL/GNN/test/test_pred_{}.csv\".format(epoch), index=0)\n",
    "        total_score = np.mean(losses)\n",
    "        print('epoch {:d}/{:d}, training mse: {:.4f} test r_2: {:.4f} mse: {:.4f}'.format(epoch + 1, n_epochs, total_score, r2, Mse), end=\"\")\n",
    "\n",
    "        return total_score\n",
    "\n",
    "\n",
    "    atom_featurizer = BaseAtomFeaturizer(\n",
    "                     {'hv': ConcatFeaturizer([\n",
    "                      partial(atom_type_one_hot, allowable_set=[\n",
    "                             'B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'Si'],\n",
    "                        encode_unknown=True),\n",
    "                      partial(atom_degree_one_hot, allowable_set=list(range(6))),\n",
    "                      atom_formal_charge, atom_num_radical_electrons,\n",
    "                      partial(atom_hybridization_one_hot, encode_unknown=True),\n",
    "                      lambda atom: [0], # A placeholder for aromatic information,\n",
    "                        atom_total_num_H_one_hot, chirality\n",
    "                     ],\n",
    "                    )})\n",
    "    bond_featurizer = BaseBondFeaturizer({\n",
    "                                         'he': lambda bond: [0 for _ in range(10)]\n",
    "        })\n",
    "\n",
    "    os.makedirs(\"./results/DL/GNN/test/\".format(data), exist_ok=True)\n",
    "    os.makedirs(\"./results/DL/GNN/train/\".format(data), exist_ok=True)\n",
    "\n",
    "    train_smi = SMILES_sample[:8000]\n",
    "    train_mols = [Chem.MolFromSmiles(smiles) for smiles in train_smi]\n",
    "    train_sol = torch.tensor(pce_sample[:8000]).reshape(-1, 1)\n",
    "\n",
    "    test_smi = SMILES_sample[8000:10000]\n",
    "    test_mols = [Chem.MolFromSmiles(smiles) for smiles in test_smi]\n",
    "    test_sol = torch.tensor(pce_sample[8000:10000]).reshape(-1, 1)\n",
    "\n",
    "    train_graph = [mol_to_bigraph(mol,\n",
    "                                  node_featurizer=atom_featurizer,\n",
    "                                  edge_featurizer=bond_featurizer) for mol in train_mols]\n",
    "\n",
    "    test_graph = [mol_to_bigraph(mol,\n",
    "                                 node_featurizer=atom_featurizer,\n",
    "                                 edge_featurizer=bond_featurizer) for mol in test_mols]\n",
    "\n",
    "    model = model_zoo.chem.AttentiveFP(node_feat_size=39,\n",
    "                                       edge_feat_size=10,\n",
    "                                       num_layers=2,\n",
    "                                       num_timesteps=2,\n",
    "                                       graph_feat_size=200,\n",
    "                                       output_size=1,\n",
    "                                       dropout=0.2)\n",
    "    model = model.to('cpu')\n",
    "\n",
    "    train_loader = DataLoader(dataset=list(zip(train_smi, train_graph, train_sol)), batch_size=16,\n",
    "                              collate_fn=collate_molgraphs)\n",
    "    test_loader = DataLoader(dataset=list(zip(test_smi, test_graph, test_sol)), batch_size=16,\n",
    "                             collate_fn=collate_molgraphs)\n",
    "\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=10 ** (-2.5), weight_decay=10 ** (-5.0),)#Adam\n",
    "    epochs = []\n",
    "    scores = []\n",
    "    r2_all = []\n",
    "    mse_all = []\n",
    "    mae_all = []\n",
    "    for e in range(n_epochs):\n",
    "        score = run_a_train_epoch(n_epochs, e, model, train_loader, loss_fn, optimizer)\n",
    "        epochs.append(e)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.plot(epochs, scores)\n",
    "    plt.savefig('111.png')\n",
    "\n",
    "    pd.DataFrame(r2_all).to_csv(\"./results/DL/GNN/test/r2_all.csv\".format(data), index=0)\n",
    "    pd.DataFrame(mse_all).to_csv(\"./results/DL/GNN/test/mse_all.csv\".format(data), index=0)\n",
    "    pd.DataFrame(mae_all).to_csv(\"./results/DL/GNN/test/mae_all.csv\".format(data), index=0)\n",
    "    pd.DataFrame(scores).to_csv(\"./results/DL/GNN/test/scores.csv\".format(data), index=0)\n",
    "    print(r2_score(test_sol, res))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(r2_all)), r2_all)\n",
    "    plt.plot(range(len(r2_all)), mse_all)\n",
    "    plt.show\n",
    "    print(max(r2_all), min(mse_all))\n",
    "    print(r2_all.index(max(r2_all)), mse_all.index(min(mse_all)))\n",
    "    \n",
    "    return r2_all, mse_all, mae_all\n",
    "\n",
    "r2_linshi, mse_linshi, mae_linshi = data_train_test(\"or_pho_pce\", n_epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5caf8a",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a161d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-02T08:09:50.949362Z",
     "start_time": "2022-04-02T08:09:50.928416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\kekule\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import os, glob, time\n",
    "import copy\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import spatial\n",
    "import os,sys, os.path\n",
    "from collections import defaultdict\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import rdkit.rdBase\n",
    "from rdkit import DataStructs\n",
    "from rdkit.DataStructs import BitVectToText\n",
    "from rdkit import DataStructs\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG\n",
    "import IPython\n",
    "\n",
    "from IPython.core.display import SVG\n",
    "from torch.autograd import Variable\n",
    "import multiprocessing\n",
    "\n",
    "import subprocess\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from rdkit.Chem import Draw\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "nets = [\"vgg19_bn\", \"densenet201\"]\n",
    "tox = [\"pce\"]\n",
    "argparser = ArgumentParser()\n",
    "args_group = argparser.add_argument_group(title='Running args')\n",
    "args_group.add_argument('-seed', type=int,  required=False, default=1)\n",
    "args_group.add_argument('-architecture', required=False, choices=nets, default=\"vgg19_bn\")\n",
    "args_group.add_argument('-lr', type=float, required=False, default=0.01)\n",
    "args_group.add_argument('-batch_size', type=int, required=False, default=16)\n",
    "args_group.add_argument('-data_augmentation', required=False, default=1,choices=[0,1] )\n",
    "args_group.add_argument('-nb_epochs_training_per_cycle',  required=False, default=200)\n",
    "args_group.add_argument('-nb_epochs_training', type=int,  required=False, default=300)\n",
    "args_group.add_argument('-load_dict', type=list,  required=False, nargs='+', default=[])\n",
    "args_group.add_argument('-Tox', type=str, required=False, default=0)\n",
    "args_group.add_argument('-cv', type=int, required=False, default=5)\n",
    "args_group.add_argument('-step_size_lr_decay', type=int, required=False, default=25)\n",
    "args_group.add_argument('-drop_factor_lr', type=float, required=False, default=0.6 )\n",
    "args = argparser.parse_args(['-batch_size', '32'\n",
    "                             , \"-architecture\"\n",
    "                             , \"densenet201\"\n",
    "                             ,'-load_dict','0','0','0','0','0'\n",
    "                             ,'-nb_epochs_training', '200'\n",
    "                             ,'-lr', '0.01'\n",
    "                             ,'-nb_epochs_training_per_cycle', '100'\n",
    "                             ,'-Tox', 'pce'\n",
    "                             ,'-cv', '5'])\n",
    "\n",
    "seed = args.seed\n",
    "net=args.architecture\n",
    "lr=args.lr\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def default_flist_reader(flist):\n",
    "    imlist = []\n",
    "    with open(flist, 'r') as rf:\n",
    "        for line in rf.readlines():\n",
    "            impath, imlabel = line.strip().split()\n",
    "            imlist.append( (impath, int(imlabel)) )\n",
    "    \n",
    "    return imlist\n",
    "\n",
    "class ImageFilelist(data.Dataset):\n",
    "    def __init__(self,  paths_labels, transform=None, target_transform=None,\n",
    "        flist_reader=default_flist_reader, loader=default_loader):\n",
    "\n",
    "        self.imlist = paths_labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        impath, target = self.imlist[index]\n",
    "        img = self.loader(impath)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "   \n",
    "        return img, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imlist)\n",
    "\n",
    "data = pd.read_csv(\"./moldata.csv\")\n",
    "SMILES = np.array(data.iloc[:,1].to_list())\n",
    "pce = data.iloc[:,4].to_list()\n",
    "del data\n",
    "\n",
    "mols_train = [Chem.MolFromSmiles(smiles) for smiles in SMILES_sample[:8000]]\n",
    "mols_test = [Chem.MolFromSmiles(smiles) for smiles in SMILES_sample[8000:]]\n",
    "\n",
    "del SMILES, pce\n",
    "\n",
    "my_smiles_train = SMILES_sample[:8000]\n",
    "my_smiles_test = SMILES_sample[8000:]\n",
    "chembl_ids_train = indexs[:8000]\n",
    "chembl_ids_test = indexs[8000:]\n",
    "activities_train = pce_sample[:8000]\n",
    "activities_test = pce_sample[8000:]\n",
    "\n",
    "if(len(my_smiles_train)+len(my_smiles_test) != len(activities_train)+len(activities_test)):\n",
    "    raise \"The number of compounds does not correspond to the number of bioactivities\"\n",
    "\n",
    "base_indices = np.arange(0,len(activities_train))\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(base_indices)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(base_indices)\n",
    "\n",
    "os.makedirs(\"./images\", exist_ok=True)\n",
    "os.makedirs(\"./images/train\", exist_ok=True)\n",
    "os.makedirs(\"./images/test\", exist_ok=True)\n",
    "\n",
    "svgs = glob.glob( \"./images/train/*.svg\")\n",
    "pngs = glob.glob( \"./images/train/*.png\")\n",
    "if len(svgs) == 0 and len(pngs) == 0:\n",
    "    for i,mm in enumerate(mols_train):\n",
    "        mol_now=[mm]\n",
    "        koko=Chem.Draw.MolsToGridImage([x for x in mol_now], molsPerRow=1,useSVG=True)\n",
    "        orig_stdout = sys.stdout\n",
    "        f = open(\"./images/train/{}.svg\".format(chembl_ids_train[i]), 'w')\n",
    "        sys.stdout = f\n",
    "        print(koko.data)\n",
    "        sys.stdout = orig_stdout\n",
    "        f.close()\n",
    "else:\n",
    "    print(\"Train SVGs ready\")\n",
    "\n",
    "svgs = glob.glob( \"./images/test/*.svg\")\n",
    "pngs = glob.glob( \"./images/test/*.png\")\n",
    "if len(svgs) == 0 and len(pngs) == 0:\n",
    "    for i,mm in enumerate(mols_test):\n",
    "        mol_now=[mm]\n",
    "        koko=Chem.Draw.MolsToGridImage([x for x in mol_now], molsPerRow=1,useSVG=True)\n",
    "        orig_stdout = sys.stdout\n",
    "        f = open(\"./images/test/{}.svg\".format(chembl_ids_test[i]), 'w')\n",
    "        sys.stdout = f\n",
    "        print(koko.data)\n",
    "        sys.stdout = orig_stdout\n",
    "        f.close()\n",
    "else:\n",
    "    print(\"Test SVGs ready\")\n",
    "\n",
    "pngs = glob.glob( \"./images/train/*.png\")\n",
    "if len(pngs) == 0:\n",
    "    basedir=os.getcwd()\n",
    "    os.chdir((\"./images/train\"))\n",
    "    cmd = \"magick mogrify -sample 224x224 -density 800 -format png *.svg\"\n",
    "    os.system(cmd)\n",
    "    os.chdir(basedir)\n",
    "else:\n",
    "    print(\"Train PNGs ready\")\n",
    "            \n",
    "pngs = glob.glob( \"./images/test/*.png\")\n",
    "if len(pngs) == 0:\n",
    "    basedir=os.getcwd()\n",
    "    os.chdir((\"./images/test\"))\n",
    "    cmd = \"magick mogrify -sample 224x224 -density 800 -format png *.svg\"\n",
    "    os.system(cmd)\n",
    "    os.chdir(basedir)\n",
    "else:\n",
    "    print(\"Test PNGs ready\")\n",
    "\n",
    "del svgs\n",
    "del pngs\n",
    "\n",
    "if args.data_augmentation == 1:\n",
    "    transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(degrees=90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            }\n",
    "else:\n",
    "    transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ]),\n",
    "            }\n",
    "\n",
    "paths_labels_train=[]\n",
    "for i,x in enumerate(activities_train):\n",
    "    path_now = './images/train/{}.png'.format(chembl_ids_train[i])\n",
    "    now = (path_now , x)\n",
    "    paths_labels_train.append(now)\n",
    "\n",
    "paths_labels_test=[]\n",
    "for i,x in enumerate(activities_test):\n",
    "    path_now = './images/test/{}.png'.format(chembl_ids_test[i])\n",
    "    now = (path_now , x)\n",
    "    paths_labels_test.append(now)                                               \n",
    "     \n",
    "workers=multiprocessing.cpu_count()\n",
    "workers = 0\n",
    "shuffle=False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    print(\"-\"*20)\n",
    "    print(\"Strat Training {} {} cv: {}\".format(args.cell_line, args.Tox, cv+1))\n",
    "    print(net)\n",
    "    print(\"data_augmentation: {}\".format(args.data_augmentation))\n",
    "    print(\"batch_size: {}\".format(args.batch_size))\n",
    "    start_epoch = -1\n",
    "    best_epoch = 0\n",
    "    load_epoch = int(args.load_dict[cv][0])\n",
    "    if  load_epoch > 0:\n",
    "        checkpoint = torch.load(\"./models/checkpoint/{}/{}/{}/{}_{}.pth\".format(args.cell_line,args.seed,net,load_epoch, cv))\n",
    "        print(\"strat from\",checkpoint[\"epoch\"],\"epoch\")\n",
    "        \n",
    "        best_epoch = load_epoch\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        scheduler.load_state_dict(checkpoint['lr_schedule'])\n",
    "        net_dict = copy.deepcopy(checkpoint['net'])\n",
    "        model.load_state_dict(net_dict)\n",
    "        del net_dict,checkpoint\n",
    "        \n",
    "    print(\"-\"*20)\n",
    "    model.cuda()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_mse = 1000.0\n",
    "    best_r2 = -10000\n",
    "    early = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch+1, num_epochs):\n",
    "        time_epoch = time.time()\n",
    "\n",
    "        # cyclical learning rate\n",
    "        if early % args.nb_epochs_training_per_cycle == 0:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "        \n",
    "        print(\"-\"*20)\n",
    "        print('Epoch {}/{} early:{}'.format(epoch, num_epochs - 1, early))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_losses=0.0\n",
    "            deno=0.0\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        preds=outputs.squeeze(1)\n",
    "                        preds = preds.type(torch.FloatTensor)\n",
    "                        loss = criterion(preds, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    del inputs, outputs, labels\n",
    "                    epoch_losses += loss.item() * len(preds)\n",
    "                    deno +=len(preds)\n",
    "                    del preds\n",
    "                    \n",
    "                epoch_loss = epoch_losses / deno\n",
    "                train_loss.append(epoch_loss)\n",
    "                print('{} Loss: {:.4f} {}'.format(phase, epoch_loss, deno))\n",
    "                \n",
    "            if phase == 'val':\n",
    "                model.eval()\n",
    "                pred=[]\n",
    "                obs=[]\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    outputs = model(inputs)\n",
    "                    for i in range(len(labels)):\n",
    "                        pred.append(float(outputs.data[i]))\n",
    "                        obs.append(float(labels.data[i]))\n",
    "                \n",
    "                    del inputs, outputs, labels\n",
    "              \n",
    "                mse = mean_squared_error(obs, pred)\n",
    "                r2 = r2_score(obs, pred)\n",
    "                lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                print('val Loss: {:.4f} r^2: {:.4f} lr: {:.4f}'.format(mse, r2, lr))\n",
    "                lr_decay.append(lr)\n",
    "                scores_mse.append(mse)\n",
    "                scores_r2.append(r2)\n",
    "\n",
    "            if phase == 'val' and r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_mse = mse\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                early=0\n",
    "                \n",
    "                kf_pred_all.iloc[index_val, 0] = pred\n",
    "                kf_pred_all.iloc[index_val, 1] = obs\n",
    "                \n",
    "                best_epoch = epoch\n",
    "                \n",
    "        \n",
    "            if phase == 'val' and r2 < best_r2:\n",
    "                early+=1\n",
    "                \n",
    "            if phase == 'train' and early>20:\n",
    "                scheduler.step()\n",
    "\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format( (time.time() - time_epoch) // 60, (time.time() - time_epoch) % 60))\n",
    "\n",
    "    plt.plot(train_loss)\n",
    "    plt.show()\n",
    "    plt.plot(scores_r2)\n",
    "    plt.plot(scores_mse)\n",
    "    plt.show()\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best r2: {:4f}'.format(best_r2), 'Best mse: {:4f}'.format(best_mse))\n",
    "    print(\"-\"*20)\n",
    "    print(\"end\")\n",
    "    print(\"-\"*20)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"Strat Training Test  {}\".format(args.Tox))\n",
    "    print(net)\n",
    "    print(\"data_augmentation: {}\".format(args.data_augmentation))\n",
    "    print(\"batch_size: {}\".format(args.batch_size))\n",
    "    start_epoch = -1\n",
    "    best_epoch = 0\n",
    "    print(\"-\"*20)\n",
    "    model.cuda()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_mse = 1000.0\n",
    "    best_r2 = -10000\n",
    "    early = 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(start_epoch+1, num_epochs):\n",
    "        time_epoch = time.time()\n",
    "\n",
    "        # cyclical learning rate\n",
    "        if early % args.nb_epochs_training_per_cycle == 0:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "        \n",
    "        print(\"-\"*20)\n",
    "        print('Epoch {}/{} early:{}'.format(epoch, num_epochs - 1, early))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            epoch_losses=0.0\n",
    "            deno=0.0\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        preds=outputs.squeeze(1)\n",
    "                        preds = preds.type(torch.FloatTensor)\n",
    "                        loss = criterion(preds, labels)\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    del inputs, outputs, labels\n",
    "                    epoch_losses += loss.item() * len(preds)\n",
    "                    deno +=len(preds)\n",
    "                    del preds\n",
    "                    \n",
    "                epoch_loss = epoch_losses / deno\n",
    "                train_loss.append(epoch_loss)\n",
    "                print('{} Loss: {:.4f} {}'.format(phase, epoch_loss, deno))\n",
    "                \n",
    "            if phase == 'val':\n",
    "                model.eval()\n",
    "                pred=[]\n",
    "                obs=[]\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "                    labels = labels.type(torch.FloatTensor)\n",
    "                    optimizer.zero_grad()\n",
    "            \n",
    "                    outputs = model(inputs)\n",
    "                    for i in range(len(labels)):\n",
    "                        pred.append(float(outputs.data[i]))\n",
    "                        obs.append(float(labels.data[i]))\n",
    "                \n",
    "                    del inputs, outputs, labels\n",
    "\n",
    "                mse = mean_squared_error(obs, pred)\n",
    "                r2 = r2_score(obs, pred)\n",
    "                lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                print('val Loss: {:.4f} r^2: {:.4f} lr: {:.4f}'.format(mse, r2, lr))\n",
    "                lr_decay.append(lr)\n",
    "                scores_mse.append(mse)\n",
    "                scores_r2.append(r2)\n",
    "\n",
    "            if phase == 'val' and r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_mse = mse\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                early=0\n",
    "                \n",
    "                test_pred.iloc[:, 0] = pred\n",
    "                test_pred.iloc[:, 1] = obs\n",
    "                test_pred.iloc[0, 2] = best_r2\n",
    "                test_pred.iloc[0, 3] = best_mse\n",
    "        \n",
    "            if phase == 'val' and r2 < best_r2:\n",
    "                early+=1            \n",
    "                \n",
    "            if phase == 'train' and early>20:\n",
    "                scheduler.step()\n",
    "\n",
    "        print('Epoch complete in {:.0f}m {:.0f}s'.format( (time.time() - time_epoch) // 60, (time.time() - time_epoch) % 60))\n",
    "        \n",
    "        clear_out()\n",
    "        plt.plot(train_loss)\n",
    "        plt.show()\n",
    "        plt.plot(scores_mse)\n",
    "        plt.show()\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in { :.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best r2: {:4f}'.format(best_r2), 'Best mse: {:4f}'.format(best_mse))\n",
    "    print(\"-\"*20)\n",
    "    print(\"end\")\n",
    "    print(\"-\"*20)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def modelselect(net):\n",
    "    pretrained = False\n",
    "    if net == \"densenet201\": \n",
    "        model_sel = models.densenet201(pretrained=pretrained)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=1920, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_sel.classifier = classi\n",
    "\n",
    "    if net == \"vgg19_bn\": \n",
    "        model_sel = models.vgg19_bn(pretrained=pretrained)\n",
    "        modules=[]\n",
    "        modules.append( nn.Linear(in_features=25088, out_features=4096, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=4096, out_features=1000, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=1000, out_features=200, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=200, out_features=100, bias=True) )\n",
    "        modules.append( nn.ReLU(inplace=True) )\n",
    "        modules.append( nn.Dropout(p=0.5) )\n",
    "        modules.append( nn.Linear(in_features=100, out_features=1, bias=True) )\n",
    "        classi = nn.Sequential(*modules)\n",
    "        model_ft.classifier = classi\n",
    "    return model_sel\n",
    "\n",
    "k_folf = False\n",
    "for net in [\"densenet201\"]:\n",
    "    r2_all = []\n",
    "    lr_all = []\n",
    "    mse_all = []\n",
    "    kf_pred_all = pd.DataFrame(index=range(len(mols_train)), columns=[\"cv1~5\", \"True\"])\n",
    "    test_score_all = pd.DataFrame(index=list(range(len(mols_test)))+[\"r2\",\"MSE\"], columns=[\"cv1\", \"cv2\", \"cv3\", \"cv4\", \"cv5\", \"True\"])\n",
    "    train_loss_all = []\n",
    "\n",
    "    if False:\n",
    "        index = base_indices\n",
    "        step = int(len(index)/args.cv)\n",
    "        for cv in range(args.cv):\n",
    "            model_ft = modelselect(net)\n",
    "            optimizer_ft = optim.SGD(model_ft.parameters(), lr=args.lr)\n",
    "            scores_r2 = []\n",
    "            lr_decay = []\n",
    "            scores_mse = []\n",
    "            train_loss = []\n",
    "            if cv < args.cv-1:\n",
    "                index_train = np.concatenate([index[:cv*step],index[(cv+1)*step:]], axis=0)\n",
    "                index_val = index[cv*step:(cv+1)*step]\n",
    "            else: \n",
    "                index_train = index[0:cv*step]\n",
    "                index_val = index[cv*step:]\n",
    "\n",
    "            paths_labels_train_train = []\n",
    "            for i in index_train:\n",
    "                paths_labels_train_train.append(paths_labels_train[i])\n",
    "\n",
    "            paths_labels_train_val = []\n",
    "            for i in index_val:\n",
    "                paths_labels_train_val.append(paths_labels_train[i])\n",
    "                \n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                        ImageFilelist(paths_labels= paths_labels_train_train,\n",
    "                        transform=transforms['train']),\n",
    "                        batch_size=args.batch_size, shuffle=shuffle,\n",
    "                        num_workers=workers) \n",
    "\n",
    "            valloader = torch.utils.data.DataLoader(\n",
    "                        ImageFilelist(paths_labels= paths_labels_train_val,\n",
    "                        transform=transforms['val']),\n",
    "                        batch_size=args.batch_size, shuffle=shuffle,\n",
    "                        num_workers=workers) \n",
    "\n",
    "            testloader = torch.utils.data.DataLoader(\n",
    "                        ImageFilelist(paths_labels= paths_labels_test,\n",
    "                        transform=transforms['test']),\n",
    "                        batch_size=args.batch_size, shuffle=shuffle,\n",
    "                        num_workers=workers) \n",
    "\n",
    "            dataloaders = {'train': trainloader, 'val':valloader, 'test':testloader}\n",
    "            criterion = torch.nn.MSELoss()\n",
    "            exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "            model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=args.nb_epochs_training)\n",
    "\n",
    "            pred = []\n",
    "            obs = []\n",
    "            for inputs, labels in dataloaders['test']:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "                model_ft.cuda()\n",
    "                labels = labels.type(torch.FloatTensor)\n",
    "                outputs = model_ft(inputs)\n",
    "                for i in range(len(labels)):\n",
    "                    pred.append(float(outputs.data[i]))\n",
    "                    obs.append(float(labels.data[i]))\n",
    "\n",
    "                del inputs, outputs, labels\n",
    "            mse = mean_squared_error(obs, pred)\n",
    "            r2 = r2_score(obs, pred)\n",
    "            test_score_all.iloc[:len(pred),cv] = pred\n",
    "            test_score_all.iloc[:len(obs),args.cv] = obs\n",
    "            test_score_all.iloc[len(pred),cv] = r2\n",
    "            test_score_all.iloc[len(pred)+1,cv] = mse\n",
    "            print('test Loss: {:.4f} r^2: {}'.format(mse, r2))\n",
    "\n",
    "\n",
    "            r2_all.append(scores_r2)\n",
    "            lr_all.append(lr_decay)\n",
    "            mse_all.append(scores_mse)\n",
    "            train_loss_all.append(train_loss)\n",
    "        \n",
    "        os.makedirs(\"./results/DL/{}/{}/train\".format(args.seed,net), exist_ok=True)\n",
    "        os.makedirs(\"./results/DL/{}/{}/test\".format(args.seed,net), exist_ok=True)\n",
    "        pd.DataFrame(r2_all).T.to_csv(\"./results/DL/{}/{}/train/r2_all.csv\".format(args.seed,net),index=None)\n",
    "        pd.DataFrame(lr_all).T.to_csv(\"./results/DL/{}/{}/train/lr_all.csv\".format(args.seed,net),index=None)\n",
    "        pd.DataFrame(mse_all).T.to_csv(\"./results/DL/{}/{}/train/mse_all.csv\".format(args.seed,net),index=None)\n",
    "        pd.DataFrame(kf_pred_all).to_csv(\"./results/DL/{}/{}/train/kf_pred_all.csv\".format(args.seed,net),index=None)\n",
    "        pd.DataFrame(train_loss_all).T.to_csv(\"./results/DL/{}/{}/train/train_loss_all.csv\".format(args.seed,net),index=None)\n",
    "        pd.DataFrame(test_score_all).to_csv(\"./results/DL/{}/{}/test/test_score_all.csv\".format(args.seed,net),index=None)\n",
    "        \n",
    "    scores_r2 = []\n",
    "    lr_decay = []\n",
    "    scores_mse = []\n",
    "    train_loss = []\n",
    "    test_pred = pd.DataFrame(index=range(len(activities_test)), columns=[\"pred\", \"True\", \"r2\", \"mse\"])\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                ImageFilelist(paths_labels= paths_labels_train,\n",
    "                transform=transforms['train']),\n",
    "                batch_size=args.batch_size, shuffle=shuffle,\n",
    "                num_workers=workers) \n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "                ImageFilelist(paths_labels= paths_labels_test,\n",
    "                transform=transforms['val']),\n",
    "                batch_size=args.batch_size, shuffle=shuffle,\n",
    "                num_workers=workers) \n",
    "\n",
    "    dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "    model_ft = modelselect(net)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=args.lr)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=args.step_size_lr_decay, gamma=args.drop_factor_lr)\n",
    "    model_ft = train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=args.nb_epochs_training)\n",
    "\n",
    "    pd.DataFrame(scores_r2).to_csv(\"./results/DL/{}/{}/test/scores_r2.csv\".format(args.seed,net),index=None)\n",
    "    pd.DataFrame(lr_decay).to_csv(\"./results/DL/{}/{}/test/lr_decay.csv\".format(args.seed,net),index=None)\n",
    "    pd.DataFrame(scores_mse).to_csv(\"./results/DL/{}/{}/test/scores_mse.csv\".format(args.seed,net),index=None)\n",
    "    pd.DataFrame(train_loss).to_csv(\"./results/DL/{}/{}/test/train_loss.csv\".format(args.seed,net),index=None)\n",
    "    pd.DataFrame(test_pred).to_csv(\"./results/DL/{}/{}/test/test_pred.csv\".format(args.seed,net),index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c176f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
